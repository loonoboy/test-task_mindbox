# Тестовое задание Mindbox
# Я изучил базовые принципы деплоймента в Kubernetes, почитал про Deployment, Probes, Requests/Limits и Autoscaling
# Я постарался применить всё, что понял, чтобы развернуть отказоустойчивое приложение с минимальными расходами

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mindbox-app
  namespace: work
  labels:
    app: &app mindbox-app
    version: &version 1.27.5
spec:
  replicas: 4
  # Я задал 4 реплики, так как из условия сказано, что именно 4 пода справляются с пиковой нагрузкой
  # Но ниже я настрою autoscaling, чтобы ночью подов было меньше, а днем — больше
  selector:
    matchLabels:
      app: *app
      version: *version
  template:
    metadata:
      labels:
        app: *app
        version: *version
    spec:
      containers:
        - name: app
          image: myregistry/mindbox-app:1.27.5
          # Я оставил абстрактный образ, так как в условии он не был указан
          ports:
            - containerPort: 8080
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
          # Почему такие значения?
          # Из условия: в обычной работе нужно ~0.1 CPU и 128M памяти
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
          # Readiness probe: я поставил initialDelaySeconds=10, так как приложение стартует 5-10 секунд
          # Это поможет не отправлять трафик раньше времени
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
          # Liveness probe нужна, чтобы Kubernetes мог перезапустить под, если он завис
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - mindbox-app
                topologyKey: topology.kubernetes.io/zone
          # Я узнал, что с помощью podAntiAffinity можно распределить поды по зонам
          # Это нужно для отказоустойчивости: если одна зона выйдет из строя, поды останутся в других

---
apiVersion: v1
kind: Service
metadata:
  name: mindbox-app-service
  namespace: work
  labeles:
    app.kubernetes.io/name: mindbox-app
    app.kubernetes.io/version: 1.27.5
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: mindbox-app
    app.kubernetes.io/version: 1.27.5
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      name: http
  # Я сделал сервис типа ClusterIP(по дефолту), так как про внешний доступ в задании ничего не сказано
  # Но для доступа извне можно было бы использовать LoadBalancer

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mindbox-app-hpa
  namespace: work
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mindbox-app
  minReplicas: 2
  maxReplicas: 5
  # Я выбрал минимум 2 пода — чтобы ночью ресурсы не тратились зря, но полностью до 0 не выключал, ведь приложение требует тяжёлого старта
  # Максимум 5 подов — это чуть больше, чем указано в нагрузочном тесте (4), чтобы был запас
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
  # Я поставил 60% CPU как порог для масштабирования. Прочитал, что обычно используют 50-70%

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mindbox-app-pdb
  namespace: work
spec:
  minAvailable: 3
  selector:
    matchLabels:
      app: mindbox-app
  # PDB я добавил для отказоустойчивости. Это гарантирует, что при обновлении или сбое всегда будет минимум 3 работающих пода, по каждой на зону
